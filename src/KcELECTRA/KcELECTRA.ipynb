{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# KcELECTRA-base\n",
    " : 왜인진 모르겠는데 beomi/KcELECTRA-base는 글자가 깨져서 beomi/KcELECTRA-base-v2022로 실행함\n",
    "\n",
    "## KcELECTRA\n",
    "- Korean comments Efficiently Learning an Encoder that Classifies Token Replacements Accurately\n",
    "- 학습 방식\n",
    "    - 입력된 토큰을 Generator에 의해 그럴듯한 가짜 토큰으로 대체한다.\n",
    "    - 이후 토큰이 가짜 토큰인지 실제 토큰인지 이진분류 방식으로 학습한다.\n",
    "    - 이러한 이진분류는 모든 토큰에 진행되므로 기존 MLM 방식보다 훨씬 효율적이다.\n",
    "- Tokenizer\n",
    "\t- BertWordPieceTokenizer\n",
    "\n",
    "[KcELECTRA 악성댓글 분류 모델 만들기](https://www.dinolabs.ai/400)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8fbac7f2ccc8450a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 환경설정\n",
    "\n",
    "`pip install pandas scikit-learn`\n",
    "\n",
    "[data 출처: github ZIZUN/korean-malicious-comments-dataset](https://github.com/ZIZUN/korean-malicious-comments-dataset/blob/master/Dataset.csv)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45fed18176abc272"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from src.Common.common import *\n",
    "\n",
    "\"\"\"\n",
    "데이터 조작 및 분석을 위한 라이브러리\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "HuggingFace 라이브러리\n",
    "- AutoTokenizer: huggingface 모델용 tokenizer 추상 객체\n",
    "- AutoModelForSequenceClassification: huggingface 모델 추상 객체, 입력 문장 label에 맞게 분류하는 모델\n",
    "- TrainingArguments: 모델 학습 하이퍼 파라미터 객체\n",
    "- Trainer: 모델 학습용 객체\n",
    "\"\"\"\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "c = CommonObj()\n",
    "\n",
    "c.KcELECTRA_MODEL_NAME = \"beomi/KcELECTRA-base-v2022\"\n",
    "c.SAVED_MODEL_NAME = \"../../models/KcELECTRA\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T04:33:50.029582100Z",
     "start_time": "2023-08-08T04:33:48.475131300Z"
    }
   },
   "id": "3abfeabfdbf48340"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset 제작"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7aae0927bea9825b"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                                             content  lable\n0  이종석 한효주 나오는 드라마 이후로 드라마 안봤다. 2년전인가?? 좀 신선했었지. ...    0.0\n1                    씨바알..노무노무 술프노... 오늘 저녁은 꽂등심이다ㅠㅜ    0.0\n2                                           짱깨 꺼라ㅡ패쓰    0.0\n3  그들의 사생활 ~ 고인이된 설리를 위해서라도 모두 조용하길 지금 누굴 탓한다고 무슨...    1.0\n4  아무리 법이 뭣같아도 무슨 자격으로 개인의 신상정보를 불특정 다수에게 공개하는지 도...    1.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n      <th>lable</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>이종석 한효주 나오는 드라마 이후로 드라마 안봤다. 2년전인가?? 좀 신선했었지. ...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>씨바알..노무노무 술프노... 오늘 저녁은 꽂등심이다ㅠㅜ</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>짱깨 꺼라ㅡ패쓰</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>그들의 사생활 ~ 고인이된 설리를 위해서라도 모두 조용하길 지금 누굴 탓한다고 무슨...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>아무리 법이 뭣같아도 무슨 자격으로 개인의 신상정보를 불특정 다수에게 공개하는지 도...</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\\t 문자로 분리된 csv 파일 읽어오기\n",
    "\"\"\"\n",
    "c.df = pd.read_csv(\"../../datas/KcELECTRA/Dataset.csv\", sep = \"\\t\")\n",
    "\n",
    "\"\"\"\n",
    "읽어온 csv 파일의 앞 5개 row 출력\n",
    "\"\"\"\n",
    "c.df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T04:33:52.040485900Z",
     "start_time": "2023-08-08T04:33:52.014567700Z"
    }
   },
   "id": "1d00cda4a8d94c8c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## null label 제거"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cddce84f5026b263"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "1602    응애 응애 엄마 저 맘에 안들죠? ........아들 ?? \" 너 내가 우스워 보이...\n1654           토니스타크 평소 \"아이엠그루트\"라는 유행어를 부러워했다는게 학계의 정설\\t1\n1992    \"13일 현대차에 따르면 올 들어 국내 소비자들의 수입차 구매의향률이 3년 만에 하...\n2920                 에이프릴이 한마디 합니다 \"예쁜게 죄\" 구하라님 \"무기징역\"\\t1\n3720          답글 글씨체를 봐라 저게 애새끼가 쓴거냐?\"빨갱이새끼가 쓴거지 ㅁㅈㅎㅉㅉ\\t0\n3807    알겠다이기ㅋㅋ 딱 채찍쳐맞는거 좋아하는 한국식 마인드네. 노예마인드. 조금만 성공한...\n3908           이래서 스스로 걸리거든 \"죄인들이\"~ㅎㅎㅎ 재미보고 털리고 그치~~~?\\t0\n4241    아버지는 내재된 악마들을 다룰 정신적 힘을 가지고 있지 않았다.\" 이 말한마디가 사...\n4283    댓글 중 \"선동 당해서 촞불든 개돼지 홍어들도 단죄를 받아야 할 공범자들이다\"에10...\n5000    스파이 제안받고 살해 안당하는 법1. 처음에 스파이 제안을 받았을때 \"중국을 위해서...\n5521    \"국방부 \"까지 ㅡㄱ ㅐ 엿같은 ㅈ ㅣ랄주댕이...좌빨에서 ㅡ인민군대로 ㅡ가려는건가...\n5866    쌩뚱맞게 60대최반엌 치매라니 그것도 곱게 사는 사모님이- -\" 알콜중독도 아니고 ...\n6477    페미메퇘지쿵쾅년인 메갈페미들은 니들이 좋아하는 싫어요 ㄱㄱ제발부탁해~~\"일반 여성\"...\n6538    아니 ㅆㅂ 그런 \"카더라\"가 넘쳐난다고 그거에 대해서 혹시 댓글게이는 뭔가 아는거 ...\n6771    저 때 투니버스에서 코요태 짧게 인터뷰 했었는데 김종민이 \"노래는 뭐 신지가 다 하...\n6932               개 족 가튼 국방부의 \"휴기연장콜센터\"발족을 축하한다 ㅆ ㅂ..\\t0\n7199    민족적 자존심과 애국심을 갖고 국산품 이용합시다 . . . \"겸손\"한 마음으로 재산...\n7252    아나운서는 목표가 아니었지ㅋㅋ재벌하고 결혼하자마자 바로 은퇴하네ㅋㅋ무슨 인터뷰한 거...\n7270    결국 준영과 다솜은 바람을 피게되고 무인도로 떠난다에 한표 ㅋㅋㅋ 자연인이 되어 \"...\n7480    지금 연락하는 여자랑 폰섹 엄청 많이했는데만나서 호텔 들어가서침대에 서로 마주보고 ...\n7499    몽골한테 \"최근에\" 250년간 지배당하고 집단강간을 당했는데 동양피가 하나도 안섞였...\n7887    뭐 선천적으로 여성스럽거나 여자역할을 하고 싶어하는 동성애자들 그럴 수 있다고는 생...\n9666         ㄹㅇ 시발 그냥 \"다른 진로 생각해 보세요\"라고만 했어도 욕 안처 먹었지.\\t0\n9698                              간만에 이단어가 떠오르는군 \"이뭐병\"\\t0\n9875    노라조 \"형\"이란 노래로 힘들 때 위로를 받곤 했습니다. 앞으로도 노라조라는 이름으...\nName: content, dtype: object"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.null_idx = c.df[c.df.lable.isnull()].index\n",
    "c.df.loc[c.null_idx, \"content\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T04:33:53.371524600Z",
     "start_time": "2023-08-08T04:33:53.366527600Z"
    }
   },
   "id": "56879111bfb8c952"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# lable은 content의 가장 끝 문자열\n",
    "c.df.loc[c.null_idx, \"lable\"] = c.df.loc[c.null_idx, \"content\"].apply(lambda x: x[-1])\n",
    "\n",
    "# content는 \"\\t\" 앞부분까지의 문자열\n",
    "c.df.loc[c.null_idx, \"content\"] = c.df.loc[c.null_idx, \"content\"].apply(lambda x: x[:-2])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T04:33:53.975042400Z",
     "start_time": "2023-08-08T04:33:53.970188900Z"
    }
   },
   "id": "e5d6773a51142422"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   content  10000 non-null  object\n",
      " 1   lable    10000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 156.4+ KB\n"
     ]
    }
   ],
   "source": [
    "c.df = c.df.astype({\"lable\": \"int64\"})\n",
    "c.df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T04:33:54.605781200Z",
     "start_time": "2023-08-08T04:33:54.596115900Z"
    }
   },
   "id": "4e7bed2df3d0c07b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train set / Test set\n",
    "\n",
    "train_set : test_set = 80 : 20"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "975b8dd45650a0a3"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 제거 전 train_data 개수: 8000\n",
      "중복 제거 전 test_data 개수: 2000\n",
      "중복 제거 후 train_data 개수: 7992\n",
      "중복 제거 후 test_data 개수: 2000\n"
     ]
    }
   ],
   "source": [
    "c.train_data = c.df.sample(frac = 0.8, random_state = 42)\n",
    "c.test_data = c.df.drop(c.train_data.index)\n",
    "\n",
    "# dataset 개수 확인\n",
    "print(f\"중복 제거 전 train_data 개수: {len(c.train_data)}\")\n",
    "print(f\"중복 제거 전 test_data 개수: {len(c.test_data)}\")\n",
    "\n",
    "# 중복 데이터 제거\n",
    "c.train_data.drop_duplicates(subset = [\"content\"], inplace = True)\n",
    "c.test_data.drop_duplicates(subset = [\"content\"], inplace = True)\n",
    "\n",
    "# dataset 개수 확인\n",
    "print(f\"중복 제거 후 train_data 개수: {len(c.train_data)}\")\n",
    "print(f\"중복 제거 후 test_data 개수: {len(c.test_data)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T04:33:57.816675600Z",
     "start_time": "2023-08-08T04:33:57.810049100Z"
    }
   },
   "id": "2b21a52992c5c92e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f427353b73b758c0"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='../../models/KcELECTRA', vocab_size=54343, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "# c.tokenizer = AutoTokenizer.from_pretrained(c.KcELECTRA_MODEL_NAME)\n",
    "c.tokenizer = AutoTokenizer.from_pretrained(c.SAVED_MODEL_NAME)\n",
    "\n",
    "print(c.tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T04:33:59.277169700Z",
     "start_time": "2023-08-08T04:33:59.257640700Z"
    }
   },
   "id": "d50bc643b8879d4f"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "c.tokenized_train_sentences = c.tokenizer(\n",
    "\tlist(c.train_data[\"content\"]),\n",
    "\treturn_tensors = \"pt\",\n",
    "\tmax_length = 128,\n",
    "\tpadding = True,\n",
    "\ttruncation = True,\n",
    "\tadd_special_tokens = True\n",
    ")\n",
    "\n",
    "c.tokenized_test_sentences = c.tokenizer(\n",
    "\tlist(c.test_data[\"content\"]),\n",
    "\treturn_tensors = \"pt\",\n",
    "\tmax_length = 128,\n",
    "\tpadding = True,\n",
    "\ttruncation = True,\n",
    "\tadd_special_tokens = True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T04:34:08.291619Z",
     "start_time": "2023-08-08T04:34:07.863862100Z"
    }
   },
   "id": "829f7174ffd0cd47"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "['[CLS]', '국방부', '~', '~', '전화로', '휴가', '##연장', '##을', '한', '병사', '##들', '몇이나', '되는지', '공개해라', '~', '어느', '훌륭한', '집안', '##의', '자제', '##분들', '##인지도', '같이', '공개해라', '~', '~', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[2, 17047, 96, 96, 26515, 11692, 16559, 4229, 3456, 24893, 4079, 20748, 14172, 15643, 96, 8437, 13221, 9435, 4059, 13654, 9167, 15127, 8348, 15643, 96, 96, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(c.tokenized_train_sentences[0])\n",
    "print(c.tokenized_train_sentences[0].tokens)\n",
    "print(c.tokenized_train_sentences[0].ids)\n",
    "print(c.tokenized_train_sentences[0].attention_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T04:34:13.547357200Z",
     "start_time": "2023-08-08T04:34:13.540643300Z"
    }
   },
   "id": "7a3f8586cdbfcfe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset 생성"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3793237c8baba01a"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class CurseDataset(torch.utils.data.Dataset):\n",
    "\tdef __init__(self, encodings, labels):\n",
    "\t\tself.encodings = encodings\n",
    "\t\tself.labels = labels\n",
    "\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\titem = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "\t\titem[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "\t\treturn item\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.labels)\n",
    "\n",
    "c.train_dataset = CurseDataset(c.tokenized_train_sentences, c.train_data[\"lable\"].values)\n",
    "c.test_dataset = CurseDataset(c.tokenized_test_sentences, c.test_data[\"lable\"].values)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T04:34:36.052757700Z",
     "start_time": "2023-08-08T04:34:36.041218600Z"
    }
   },
   "id": "c8e43ed930e9295a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 Load"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2443f7ca8be8b536"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# c.model = AutoModelForSequenceClassification.from_pretrained(c.KcELECTRA_MODEL_NAME, num_labels = 2).to(\"cuda:0\")\n",
    "c.model = AutoModelForSequenceClassification.from_pretrained(c.SAVED_MODEL_NAME, num_labels = 2).to(\"cuda:0\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T04:34:54.160544500Z",
     "start_time": "2023-08-08T04:34:53.355840500Z"
    }
   },
   "id": "53420312e0e63c46"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Arguments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "def7d06c104cc8ea"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "c.training_args = TrainingArguments(\n",
    "\toutput_dir = \"./\",\n",
    "\tnum_train_epochs = 10,\n",
    "\tper_device_train_batch_size = 8,\n",
    "\tper_device_eval_batch_size = 64,\n",
    "\tlogging_dir = \"./logs\",\n",
    "\tlogging_steps = 500, # 학습 log 기록 단위\n",
    "\tlog_level = \"info\",\n",
    "\tsave_total_limit = 2 # 학습 결과 저장 최대 갯수\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T04:35:08.329005300Z",
     "start_time": "2023-08-08T04:35:08.315179100Z"
    }
   },
   "id": "555904dfa56b4019"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Trainer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a08effd35280bff"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "c.trainer = Trainer(\n",
    "\tmodel = c.model,\n",
    "\targs = c.training_args,\n",
    "\ttrain_dataset = c.train_dataset,\n",
    "\teval_dataset = c.test_dataset,\n",
    "\tcompute_metrics = compute_metrics\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T04:35:22.879450500Z",
     "start_time": "2023-08-08T04:35:21.921948100Z"
    }
   },
   "id": "d2e1c58b03742585"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e73c5d04ef3bf9"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jongg\\PycharmProjects\\NLP\\venv\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 7,992\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 9,990\n",
      "  Number of trainable parameters = 127,778,306\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='9990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/9990 : < :, Epoch 0.00/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./checkpoint-500\n",
      "Configuration saved in ./checkpoint-500\\config.json\n",
      "Model weights saved in ./checkpoint-500\\pytorch_model.bin\n",
      "Saving model checkpoint to ./checkpoint-1000\n",
      "Configuration saved in ./checkpoint-1000\\config.json\n",
      "Model weights saved in ./checkpoint-1000\\pytorch_model.bin\n",
      "Saving model checkpoint to ./checkpoint-1500\n",
      "Configuration saved in ./checkpoint-1500\\config.json\n",
      "Model weights saved in ./checkpoint-1500\\pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-2000\n",
      "Configuration saved in ./checkpoint-2000\\config.json\n",
      "Model weights saved in ./checkpoint-2000\\pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-2500\n",
      "Configuration saved in ./checkpoint-2500\\config.json\n",
      "Model weights saved in ./checkpoint-2500\\pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-3000\n",
      "Configuration saved in ./checkpoint-3000\\config.json\n",
      "Model weights saved in ./checkpoint-3000\\pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-3500\n",
      "Configuration saved in ./checkpoint-3500\\config.json\n",
      "Model weights saved in ./checkpoint-3500\\pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-4000\n",
      "Configuration saved in ./checkpoint-4000\\config.json\n",
      "Model weights saved in ./checkpoint-4000\\pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-4500\n",
      "Configuration saved in ./checkpoint-4500\\config.json\n",
      "Model weights saved in ./checkpoint-4500\\pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-5000\n",
      "Configuration saved in ./checkpoint-5000\\config.json\n",
      "Model weights saved in ./checkpoint-5000\\pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-5500\n",
      "Configuration saved in ./checkpoint-5500\\config.json\n",
      "Model weights saved in ./checkpoint-5500\\pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-6000\n",
      "Configuration saved in ./checkpoint-6000\\config.json\n",
      "Model weights saved in ./checkpoint-6000\\pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-6500\n",
      "Configuration saved in ./checkpoint-6500\\config.json\n",
      "Model weights saved in ./checkpoint-6500\\pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-7000\n",
      "Configuration saved in ./checkpoint-7000\\config.json\n",
      "Model weights saved in ./checkpoint-7000\\pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-7500\n",
      "Configuration saved in ./checkpoint-7500\\config.json\n",
      "Model weights saved in ./checkpoint-7500\\pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-8000\n",
      "Configuration saved in ./checkpoint-8000\\config.json\n",
      "Model weights saved in ./checkpoint-8000\\pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-8500\n",
      "Configuration saved in ./checkpoint-8500\\config.json\n",
      "Model weights saved in ./checkpoint-8500\\pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-9000\n",
      "Configuration saved in ./checkpoint-9000\\config.json\n",
      "Model weights saved in ./checkpoint-9000\\pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-9500\n",
      "Configuration saved in ./checkpoint-9500\\config.json\n",
      "Model weights saved in ./checkpoint-9500\\pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint-8500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in ../../models/KcELECTRA\\config.json\n",
      "Model weights saved in ../../models/KcELECTRA\\pytorch_model.bin\n",
      "tokenizer config file saved in ../../models/KcELECTRA\\tokenizer_config.json\n",
      "Special tokens file saved in ../../models/KcELECTRA\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": "('../../models/KcELECTRA\\\\tokenizer_config.json',\n '../../models/KcELECTRA\\\\special_tokens_map.json',\n '../../models/KcELECTRA\\\\vocab.txt',\n '../../models/KcELECTRA\\\\added_tokens.json',\n '../../models/KcELECTRA\\\\tokenizer.json')"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.trainer.train()\n",
    "\n",
    "c.model.save_pretrained(c.SAVED_MODEL_NAME)\n",
    "c.tokenizer.save_pretrained(c.SAVED_MODEL_NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T04:59:03.877582300Z",
     "start_time": "2023-08-08T04:35:35.680044300Z"
    }
   },
   "id": "f7fe5a14ab068133"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Evaluate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e92255dc0d221627"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/32 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'eval_loss': 1.0783494710922241,\n 'eval_accuracy': 0.8935,\n 'eval_f1': 0.8949185989146522,\n 'eval_precision': 0.8712776176753122,\n 'eval_recall': 0.9198782961460447,\n 'eval_runtime': 7.7176,\n 'eval_samples_per_second': 259.149,\n 'eval_steps_per_second': 4.146,\n 'epoch': 10.0}"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.trainer.evaluate(eval_dataset = c.test_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T04:59:11.599294200Z",
     "start_time": "2023-08-08T04:59:03.875802300Z"
    }
   },
   "id": "5e69e959a6de9740"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 적용"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f2ffc711b5cdb19"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# 0: 악성댓글 1: 정상댓글\n",
    "def sentence_predict(sent):\n",
    "\t# 평가모드\n",
    "\tc.model.eval()\n",
    "\t\n",
    "\t# 입력된 문장 tokenizing\n",
    "\ttokenized_sent = c.tokenizer(sent,\n",
    "\t\t\t\t\t\t\t   return_tensors = \"pt\",\n",
    "\t\t\t\t\t\t\t   truncation = True,\n",
    "\t\t\t\t\t\t\t   add_special_tokens = True,\n",
    "\t\t\t\t\t\t\t   max_length = 128)\n",
    "\t\n",
    "\ttokenized_sent.to(\"cuda:0\")\n",
    "\t\n",
    "\t# 예측\n",
    "\twith torch.no_grad():\n",
    "\t\toutputs = c.model(**tokenized_sent)\n",
    "\t\n",
    "\t# 결과 return\n",
    "\tlogits = outputs[0]\n",
    "\tlogits = logits.detach().cpu()\n",
    "\tresult = logits.argmax(-1)\n",
    "\t\n",
    "\treturn \"정상댓글\" if result == 1 else \"악성댓글\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T04:59:11.600298200Z",
     "start_time": "2023-08-08T04:59:11.597827100Z"
    }
   },
   "id": "3c982761c594e5e"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력한 댓글 \"염병할 놈들\"는 악성댓글입니다.\n"
     ]
    }
   ],
   "source": [
    "sentence = input(\"댓글입력: \")\n",
    "print(f\"입력한 댓글 \\\"{sentence}\\\"는 {sentence_predict(sentence)}입니다.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T04:59:19.810267900Z",
     "start_time": "2023-08-08T04:59:11.601298200Z"
    }
   },
   "id": "a7877a81f347b9de"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/*====================================================================*/\n",
      "device:0\n",
      "NVIDIA GeForce RTX 3060\n",
      "\n",
      "GPU memory occupied: 9059 MB.\n",
      "\n",
      "Allocated GPU Memory: 1.46GB\n",
      "Reserved GPU Memory: 1.63GB\n",
      "/*====================================================================*/\n"
     ]
    }
   ],
   "source": [
    "del c"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T04:59:19.927667300Z",
     "start_time": "2023-08-08T04:59:19.807268200Z"
    }
   },
   "id": "b8b3b4c35cdc3342"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "732c8e6793807bba"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
